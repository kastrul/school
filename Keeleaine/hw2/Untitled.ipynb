{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_data = [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0]\n",
    "vector = torch.Tensor(v_data)\n",
    "x = Variable(vector, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 10\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y = x.max()\n",
    "print(y.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 2\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target = Variable(torch.Tensor([12]), requires_grad=True)\n",
    "error = torch.abs(y - target)\n",
    "print(error.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the function in the form `f(x) = max(x) - 12`, where `f(x)` is `error`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets use `error.backward()` to compute all the gradient values in relation to the absolute error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      "-1\n",
      "[torch.FloatTensor of size 10]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "error.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is correct because the last element in the array was the maximum so only that will have an effect on the error, and it is 1 because when that element changes by one, so will the error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `vocab_size` - number of input words\n",
    "* `embedding_dim` - dimension of a word vector\n",
    "* `dropout_prob` - how large part of the layer is being used\n",
    "* `num_classes` - number of possible outputs\n",
    "* `sum_rows(x)` - returns the sum of rows, i.e. sum of word embeddings \n",
    "* `mean_rows(x)` - similar to sum_rows, but also divides with the number of rows/words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CbowSum(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes, vocab_size, embedding_dim, dropout_prob):\n",
    "        super(CbowSum, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_dim)      \n",
    "        self.lin1 = nn.Linear(embedding_dim, 100)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.fc = nn.Linear(100, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)        \n",
    "        x = self.sum_rows(x)\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = self.dropout(x)        \n",
    "        return self.fc(x)    \n",
    "    \n",
    "    def sum_rows(self, x):\n",
    "        return torch.sum(x, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CbowMean(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes, vocab_size, embedding_dim, dropout_prob):\n",
    "        super(CbowMean, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_dim)      \n",
    "        self.lin1 = nn.Linear(embedding_dim, 100)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.fc = nn.Linear(100, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)        \n",
    "        x = self.mean_rows(x)\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = self.dropout(x)        \n",
    "        return self.fc(x)    \n",
    "    \n",
    "    def mean_rows(self, x):\n",
    "        n_rows = x.shape[0]\n",
    "        x = torch.sum(x, 1)\n",
    "        return torch.div(x, n_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_interpolated(data_iter, models):\n",
    "\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "        \n",
    "    corrects, avg_loss = 0, 0\n",
    "    for batch in data_iter:\n",
    "        text, target = batch.text, batch.label\n",
    "        probabilities = 0\n",
    "\n",
    "        # subtract one from label ID because we don't have <unk> labels\n",
    "        target -= 1\n",
    "        for model in models:\n",
    "            probabilities = probabilities + F.softmax(model(text), dim=1)\n",
    "        probabilities = probabilities/len(models)\n",
    "        \n",
    "        loss = F.cross_entropy(probabilities, target, size_average=False)\n",
    "                                   \n",
    "        avg_loss += loss.data[0]\n",
    "        corrects += (torch.max(probabilities, 1)[1].view(target.size()).data \\\n",
    "                     == target.data).sum()\n",
    "\n",
    "    size = len(data_iter.dataset)\n",
    "    avg_loss /= size\n",
    "    accuracy = 100.0 * corrects/size\n",
    "    print('\\nEvaluation - loss: {:.6f}  acc: {:.4f}%({}/{}) \\n'.format(avg_loss, \n",
    "                                                                     accuracy, \n",
    "                                                                     corrects, \n",
    "                                                                     size))\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CnnText(nn.Module):\n",
    "  \n",
    "    def __init__(self, num_classes, vocab_size, embedding_dim, dropout_prob):\n",
    "        super(CnnText, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.conv1 = nn.Conv1d(embedding_dim, 32, kernel_size=3, stride=1)\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=3, stride=1)\n",
    "        self.conv3 = nn.Conv1d(64, 64, kernel_size=3, stride=1)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.fc = nn.Linear(64, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Conv1d takes in (batch, channels, seq_len), but raw embedded is (batch, seq_len, channels)\n",
    "        x = self.embed(x).permute(0, 2, 1)\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        #print(x.shape)\n",
    "        x = F.max_pool1d(x, 2)\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        #print(x.shape)\n",
    "        x = F.max_pool1d(x, x.size(2))\n",
    "        #print(x.shape)\n",
    "        x = x.view(-1, 64)\n",
    "        #print(x.shape)\n",
    "        x = self.dropout(x) \n",
    "        logit = self.fc(x)\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
